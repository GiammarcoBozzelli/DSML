{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM049PJ+zGLeyY9Wcp8tw/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiammarcoBozzelli/DSML/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OUfTalBPuvHy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from random import randint\n",
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv\"\n",
        "data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# augmented_df = pd.read_csv('https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/augmented_dataset.csv')"
      ],
      "metadata": {
        "id": "T_27XXwPmdE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers[torch] accelerate -U"
      ],
      "metadata": {
        "id": "S3Z0K3DA2UvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distilbert"
      ],
      "metadata": {
        "id": "-H6OEuWBL8Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from random import randint\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
        "data['difficulty'] = data['difficulty'].map(label_mapping)\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def get_sentence_length(sentence):\n",
        "    return len(sentence)\n",
        "\n",
        "def get_word_count(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return len(words)\n",
        "\n",
        "def get_avg_word_length(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return np.mean([len(word) for word in words])\n",
        "\n",
        "def count_punctuation(sentence):\n",
        "    return sum([1 for char in sentence if char in string.punctuation])\n",
        "\n",
        "# Extract Features\n",
        "data['sentence_length'] = data['sentence'].apply(get_sentence_length)\n",
        "data['word_count'] = data['sentence'].apply(get_word_count)\n",
        "data['avg_word_length'] = data['sentence'].apply(get_avg_word_length)\n",
        "data['punctuation_count'] = data['sentence'].apply(count_punctuation)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "feature_cols = ['sentence_length', 'word_count', 'avg_word_length', 'punctuation_count']\n",
        "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "train_encodings = tokenizer(X_train['sentence'].tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None, features=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        if self.features is not None:\n",
        "            item['features'] = torch.tensor(self.features[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "\n",
        "# Cross-validation with KFold\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Training fold {fold + 1}\")\n",
        "\n",
        "    # Prepare training and validation datasets\n",
        "    train_data = data.iloc[train_idx]\n",
        "    val_data = data.iloc[val_idx]\n",
        "    # Load pre-trained model\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=6).to(device)\n",
        "\n",
        "    train_encodings_fold = tokenizer(train_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "    val_encodings_fold = tokenizer(val_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    train_dataset = ClassificationDataset(train_encodings_fold, train_data['difficulty'].tolist(), train_data[feature_cols].values)\n",
        "    val_dataset = ClassificationDataset(val_encodings_fold, val_data['difficulty'].tolist(), val_data[feature_cols].values)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold + 1}',\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=1000,\n",
        "        weight_decay=0.0015,\n",
        "        logging_dir=f'./logs_fold_{fold + 1}',\n",
        "        logging_steps=20,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=0.00005,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    # Define trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions, label_ids, metrics = trainer.predict(val_dataset)\n",
        "    preds = predictions.argmax(-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(val_data['difficulty'], preds)\n",
        "    precision = precision_score(val_data['difficulty'], preds, average='weighted')\n",
        "    recall = recall_score(val_data['difficulty'], preds, average='weighted')\n",
        "    f1 = f1_score(val_data['difficulty'], preds, average='weighted')\n",
        "\n",
        "    # Store metrics\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Fold {fold + 1} - Accuracy: {accuracy:.2f}')\n",
        "    print(f'Fold {fold + 1} - Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} - Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} - F1 Score: {f1:.2f}')\n",
        "\n",
        "# Print the average results across folds\n",
        "print(f'Average Accuracy: {np.mean(accuracy_list):.2f}')\n",
        "print(f'Average Precision: {np.mean(precision_list):.2f}')\n",
        "print(f'Average Recall: {np.mean(recall_list):.2f}')\n",
        "print(f'Average F1 Score: {np.mean(f1_list):.2f}')\n",
        "\n",
        "# Alternatively, print a detailed classification report for the final fold\n",
        "print(classification_report(val_data['difficulty'], preds, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WzM4oY_z1oi2",
        "outputId": "11e3138b-1c37-4b87-fce9-d19bdc680184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2560' max='2560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2560/2560 06:31, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.299600</td>\n",
              "      <td>1.238442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.189300</td>\n",
              "      <td>1.190658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.066300</td>\n",
              "      <td>1.236999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.802100</td>\n",
              "      <td>1.295088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.379000</td>\n",
              "      <td>1.553977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.089100</td>\n",
              "      <td>2.290314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.176600</td>\n",
              "      <td>2.572154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.008500</td>\n",
              "      <td>2.728797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Accuracy: 0.55\n",
            "Fold 1 - Precision: 0.58\n",
            "Fold 1 - Recall: 0.55\n",
            "Fold 1 - F1 Score: 0.55\n",
            "Training fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2560' max='2560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2560/2560 05:57, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.243300</td>\n",
              "      <td>1.292821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.125900</td>\n",
              "      <td>1.190287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.977500</td>\n",
              "      <td>1.269662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.595600</td>\n",
              "      <td>1.421422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.408800</td>\n",
              "      <td>1.986328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.160800</td>\n",
              "      <td>2.612634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.051900</td>\n",
              "      <td>2.999761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>3.057564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 - Accuracy: 0.50\n",
            "Fold 2 - Precision: 0.52\n",
            "Fold 2 - Recall: 0.50\n",
            "Fold 2 - F1 Score: 0.51\n",
            "Training fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2560' max='2560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2560/2560 06:19, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.311700</td>\n",
              "      <td>1.314149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.219400</td>\n",
              "      <td>1.216400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.065800</td>\n",
              "      <td>1.159325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.857300</td>\n",
              "      <td>1.320384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.423800</td>\n",
              "      <td>1.665996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.258400</td>\n",
              "      <td>2.245732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.091200</td>\n",
              "      <td>2.606750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.052300</td>\n",
              "      <td>2.633737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Accuracy: 0.54\n",
            "Fold 3 - Precision: 0.55\n",
            "Fold 3 - Recall: 0.54\n",
            "Fold 3 - F1 Score: 0.54\n",
            "Average Accuracy: 0.53\n",
            "Average Precision: 0.55\n",
            "Average Recall: 0.53\n",
            "Average F1 Score: 0.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.66      0.66      0.66       205\n",
            "          A2       0.50      0.49      0.50       214\n",
            "          B1       0.45      0.49      0.47       216\n",
            "          B2       0.53      0.51      0.52       221\n",
            "          C1       0.49      0.53      0.51       208\n",
            "          C2       0.66      0.58      0.62       216\n",
            "\n",
            "    accuracy                           0.54      1280\n",
            "   macro avg       0.55      0.54      0.55      1280\n",
            "weighted avg       0.55      0.54      0.54      1280\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./distilbert')\n",
        "tokenizer.save_pretrained('./distilbert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp1KePlUIEcH",
        "outputId": "d4d5924c-b9f0-4d28-a747-0e5c2d36dbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./distilbert/tokenizer_config.json',\n",
              " './distilbert/special_tokens_map.json',\n",
              " './distilbert/vocab.txt',\n",
              " './distilbert/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./distilbert.zip ./distilbert/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKGVOfyPQVep",
        "outputId": "a6a406b0-3630-4d61-a6db-44f983268b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: distilbert/ (stored 0%)\n",
            "  adding: distilbert/config.json (deflated 52%)\n",
            "  adding: distilbert/vocab.txt (deflated 45%)\n",
            "  adding: distilbert/special_tokens_map.json (deflated 42%)\n",
            "  adding: distilbert/model.safetensors (deflated 7%)\n",
            "  adding: distilbert/tokenizer_config.json (deflated 75%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camembert"
      ],
      "metadata": {
        "id": "g-ZOASazJs31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import CamembertTokenizer,CamembertForSequenceClassification, CamembertModel, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from random import randint\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
        "data['difficulty'] = data['difficulty'].map(label_mapping)\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def get_sentence_length(sentence):\n",
        "    return len(sentence)\n",
        "\n",
        "def get_word_count(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return len(words)\n",
        "\n",
        "def get_avg_word_length(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return np.mean([len(word) for word in words])\n",
        "\n",
        "def count_punctuation(sentence):\n",
        "    return sum([1 for char in sentence if char in string.punctuation])\n",
        "\n",
        "# Extract Features\n",
        "data['sentence_length'] = data['sentence'].apply(get_sentence_length)\n",
        "data['word_count'] = data['sentence'].apply(get_word_count)\n",
        "data['avg_word_length'] = data['sentence'].apply(get_avg_word_length)\n",
        "data['punctuation_count'] = data['sentence'].apply(count_punctuation)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "feature_cols = ['sentence_length', 'word_count', 'avg_word_length', 'punctuation_count']\n",
        "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "train_encodings = tokenizer(X_train['sentence'].tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None, features=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        if self.features is not None:\n",
        "            item['features'] = torch.tensor(self.features[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Cross-validation with KFold\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Training fold {fold + 1}\")\n",
        "\n",
        "    model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6).to(device)\n",
        "\n",
        "    # Prepare training and validation datasets\n",
        "    train_data = data.iloc[train_idx]\n",
        "    val_data = data.iloc[val_idx]\n",
        "\n",
        "    train_encodings_fold = tokenizer(train_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "    val_encodings_fold = tokenizer(val_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    train_dataset = ClassificationDataset(train_encodings_fold, train_data['difficulty'].tolist(), train_data[feature_cols].values)\n",
        "    val_dataset = ClassificationDataset(val_encodings_fold, val_data['difficulty'].tolist(), val_data[feature_cols].values)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold + 1}',\n",
        "        num_train_epochs=7,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=1000,\n",
        "        weight_decay=0.0015,\n",
        "        logging_dir=f'./logs_fold_{fold + 1}',\n",
        "        logging_steps=20,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=0.00001,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    # Define trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions, label_ids, metrics = trainer.predict(val_dataset)\n",
        "    preds = predictions.argmax(-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(val_data['difficulty'], preds)\n",
        "    precision = precision_score(val_data['difficulty'], preds, average='weighted')\n",
        "    recall = recall_score(val_data['difficulty'], preds, average='weighted')\n",
        "    f1 = f1_score(val_data['difficulty'], preds, average='weighted')\n",
        "\n",
        "    # Store metrics\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Fold {fold + 1} - Accuracy: {accuracy:.2f}')\n",
        "    print(f'Fold {fold + 1} - Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} - Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} - F1 Score: {f1:.2f}')\n",
        "\n",
        "# Print the average results across folds\n",
        "print(f'Average Accuracy: {np.mean(accuracy_list):.2f}')\n",
        "print(f'Average Precision: {np.mean(precision_list):.2f}')\n",
        "print(f'Average Recall: {np.mean(recall_list):.2f}')\n",
        "print(f'Average F1 Score: {np.mean(f1_list):.2f}')\n",
        "\n",
        "# Alternatively, print a detailed classification report for the final fold\n",
        "print(classification_report(val_data['difficulty'], preds, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GK8lNJXQMBtd",
        "outputId": "994662b3-88ae-46d0-b4d0-d82ce66a3ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2240' max='2240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2240/2240 06:59, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.766800</td>\n",
              "      <td>1.755382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.553200</td>\n",
              "      <td>1.485113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.348500</td>\n",
              "      <td>1.272319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.183100</td>\n",
              "      <td>1.187115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.001700</td>\n",
              "      <td>1.096363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.962900</td>\n",
              "      <td>1.053249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.916800</td>\n",
              "      <td>1.051031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Accuracy: 0.57\n",
            "Fold 1 - Precision: 0.57\n",
            "Fold 1 - Recall: 0.57\n",
            "Fold 1 - F1 Score: 0.56\n",
            "Training fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2240' max='2240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2240/2240 06:20, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.767000</td>\n",
              "      <td>1.769972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.537200</td>\n",
              "      <td>1.496732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.293400</td>\n",
              "      <td>1.270783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>1.209515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.922000</td>\n",
              "      <td>1.176997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.872600</td>\n",
              "      <td>1.182077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.887000</td>\n",
              "      <td>1.186372</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 - Accuracy: 0.49\n",
            "Fold 2 - Precision: 0.50\n",
            "Fold 2 - Recall: 0.49\n",
            "Fold 2 - F1 Score: 0.48\n",
            "Training fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2240' max='2240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2240/2240 06:54, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.769100</td>\n",
              "      <td>1.768578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.564000</td>\n",
              "      <td>1.522200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.321700</td>\n",
              "      <td>1.313596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.199200</td>\n",
              "      <td>1.166784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.055300</td>\n",
              "      <td>1.098279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.939400</td>\n",
              "      <td>1.097569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.825200</td>\n",
              "      <td>1.109191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Accuracy: 0.52\n",
            "Fold 3 - Precision: 0.52\n",
            "Fold 3 - Recall: 0.52\n",
            "Fold 3 - F1 Score: 0.51\n",
            "Average Accuracy: 0.53\n",
            "Average Precision: 0.53\n",
            "Average Recall: 0.53\n",
            "Average F1 Score: 0.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.61      0.81      0.70       205\n",
            "          A2       0.48      0.43      0.45       214\n",
            "          B1       0.46      0.48      0.47       216\n",
            "          B2       0.44      0.53      0.48       221\n",
            "          C1       0.42      0.29      0.35       208\n",
            "          C2       0.70      0.59      0.64       216\n",
            "\n",
            "    accuracy                           0.52      1280\n",
            "   macro avg       0.52      0.52      0.51      1280\n",
            "weighted avg       0.52      0.52      0.51      1280\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./camembert')\n",
        "tokenizer.save_pretrained('./camembert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evXY6icOBQif",
        "outputId": "c4f20784-90f7-4749-a5de-6bc6ff40fc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./camembert/tokenizer_config.json',\n",
              " './camembert/special_tokens_map.json',\n",
              " './camembert/sentencepiece.bpe.model',\n",
              " './camembert/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./camembert.zip ./camembert/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G10VujTyQ9XM",
        "outputId": "af1fd8af-6995-4d2c-e8df-43eb0fe675d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: camembert/ (stored 0%)\n",
            "  adding: camembert/config.json (deflated 55%)\n",
            "  adding: camembert/sentencepiece.bpe.model (deflated 49%)\n",
            "  adding: camembert/special_tokens_map.json (deflated 52%)\n",
            "  adding: camembert/model.safetensors (deflated 12%)\n",
            "  adding: camembert/tokenizer_config.json (deflated 81%)\n",
            "  adding: camembert/added_tokens.json (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flaubert"
      ],
      "metadata": {
        "id": "JfQ5GHoGJz3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from random import randint\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
        "data['difficulty'] = data['difficulty'].map(label_mapping)\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def get_sentence_length(sentence):\n",
        "    return len(sentence)\n",
        "\n",
        "def get_word_count(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return len(words)\n",
        "\n",
        "def get_avg_word_length(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return np.mean([len(word) for word in words])\n",
        "\n",
        "def count_punctuation(sentence):\n",
        "    return sum([1 for char in sentence if char in string.punctuation])\n",
        "\n",
        "# Extract Features\n",
        "data['sentence_length'] = data['sentence'].apply(get_sentence_length)\n",
        "data['word_count'] = data['sentence'].apply(get_word_count)\n",
        "data['avg_word_length'] = data['sentence'].apply(get_avg_word_length)\n",
        "data['punctuation_count'] = data['sentence'].apply(count_punctuation)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "feature_cols = ['sentence_length', 'word_count', 'avg_word_length', 'punctuation_count']\n",
        "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
        "\n",
        "train_encodings = tokenizer(X_train['sentence'].tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None, features=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        if self.features is not None:\n",
        "            item['features'] = torch.tensor(self.features[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Load pre-trained model\n",
        "\n",
        "\n",
        "# Cross-validation with KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Training fold {fold + 1}\")\n",
        "\n",
        "    # Prepare training and validation datasets\n",
        "    train_data = data.iloc[train_idx]\n",
        "    val_data = data.iloc[val_idx]\n",
        "\n",
        "    model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=6).to(device)\n",
        "\n",
        "    train_encodings_fold = tokenizer(train_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "    val_encodings_fold = tokenizer(val_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    train_dataset = ClassificationDataset(train_encodings_fold, train_data['difficulty'].tolist(), train_data[feature_cols].values)\n",
        "    val_dataset = ClassificationDataset(val_encodings_fold, val_data['difficulty'].tolist(), val_data[feature_cols].values)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold + 1}',\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=1000,\n",
        "        weight_decay=0.0015,\n",
        "        logging_dir=f'./logs_fold_{fold + 1}',\n",
        "        logging_steps=20,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=0.00001,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    # Define trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions, label_ids, metrics = trainer.predict(val_dataset)\n",
        "    preds = predictions.argmax(-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(val_data['difficulty'], preds)\n",
        "    precision = precision_score(val_data['difficulty'], preds, average='weighted')\n",
        "    recall = recall_score(val_data['difficulty'], preds, average='weighted')\n",
        "    f1 = f1_score(val_data['difficulty'], preds, average='weighted')\n",
        "\n",
        "    # Store metrics\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Fold {fold + 1} - Accuracy: {accuracy:.2f}')\n",
        "    print(f'Fold {fold + 1} - Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} - Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} - F1 Score: {f1:.2f}')\n",
        "\n",
        "# Print the average results across folds\n",
        "print(f'Average Accuracy: {np.mean(accuracy_list):.2f}')\n",
        "print(f'Average Precision: {np.mean(precision_list):.2f}')\n",
        "print(f'Average Recall: {np.mean(recall_list):.2f}')\n",
        "print(f'Average F1 Score: {np.mean(f1_list):.2f}')\n",
        "\n",
        "# Alternatively, print a detailed classification report for the final fold\n",
        "print(classification_report(val_data['difficulty'], preds, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KMdolGwgJ1dn",
        "outputId": "65f55421-50fe-4ed0-cbf1-1397dfc77956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3840/3840 10:54, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.772200</td>\n",
              "      <td>1.733193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.622100</td>\n",
              "      <td>1.487104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.146700</td>\n",
              "      <td>1.209727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.239000</td>\n",
              "      <td>1.133424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.045000</td>\n",
              "      <td>1.099472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.027000</td>\n",
              "      <td>1.054684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.939800</td>\n",
              "      <td>1.057798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.883500</td>\n",
              "      <td>1.077912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.803600</td>\n",
              "      <td>1.094515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.651900</td>\n",
              "      <td>1.101794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Accuracy: 0.59\n",
            "Fold 1 - Precision: 0.59\n",
            "Fold 1 - Recall: 0.59\n",
            "Fold 1 - F1 Score: 0.59\n",
            "Training fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3840/3840 10:43, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.743300</td>\n",
              "      <td>1.676652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.428200</td>\n",
              "      <td>1.421647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.209700</td>\n",
              "      <td>1.169028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.091700</td>\n",
              "      <td>1.065796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.950800</td>\n",
              "      <td>1.059872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.005600</td>\n",
              "      <td>1.081518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.869900</td>\n",
              "      <td>1.082840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.839400</td>\n",
              "      <td>1.130252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.836500</td>\n",
              "      <td>1.099546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.715700</td>\n",
              "      <td>1.112669</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 - Accuracy: 0.57\n",
            "Fold 2 - Precision: 0.58\n",
            "Fold 2 - Recall: 0.57\n",
            "Fold 2 - F1 Score: 0.57\n",
            "Training fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3840/3840 10:54, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.693700</td>\n",
              "      <td>1.699808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.493300</td>\n",
              "      <td>1.452464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.192200</td>\n",
              "      <td>1.273288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.146100</td>\n",
              "      <td>1.200674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.820400</td>\n",
              "      <td>1.244063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.944700</td>\n",
              "      <td>1.161670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.835400</td>\n",
              "      <td>1.226576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.724700</td>\n",
              "      <td>1.256227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.779300</td>\n",
              "      <td>1.217264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.716100</td>\n",
              "      <td>1.209977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Accuracy: 0.53\n",
            "Fold 3 - Precision: 0.53\n",
            "Fold 3 - Recall: 0.53\n",
            "Fold 3 - F1 Score: 0.53\n",
            "Training fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3840/3840 10:31, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.775700</td>\n",
              "      <td>1.671537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.553800</td>\n",
              "      <td>1.471448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.243400</td>\n",
              "      <td>1.244058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.048100</td>\n",
              "      <td>1.152903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.084600</td>\n",
              "      <td>1.112401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.902800</td>\n",
              "      <td>1.119035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.740300</td>\n",
              "      <td>1.150162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.737000</td>\n",
              "      <td>1.175533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.734400</td>\n",
              "      <td>1.180974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.794200</td>\n",
              "      <td>1.206400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 - Accuracy: 0.54\n",
            "Fold 4 - Precision: 0.55\n",
            "Fold 4 - Recall: 0.54\n",
            "Fold 4 - F1 Score: 0.54\n",
            "Training fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3840/3840 10:45, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.757800</td>\n",
              "      <td>1.695904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.493100</td>\n",
              "      <td>1.445942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.325500</td>\n",
              "      <td>1.207282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.251800</td>\n",
              "      <td>1.158775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.103300</td>\n",
              "      <td>1.062548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.927600</td>\n",
              "      <td>1.106098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.982600</td>\n",
              "      <td>1.096968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.896500</td>\n",
              "      <td>1.128776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.724600</td>\n",
              "      <td>1.151782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>1.103826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 - Accuracy: 0.56\n",
            "Fold 5 - Precision: 0.56\n",
            "Fold 5 - Recall: 0.56\n",
            "Fold 5 - F1 Score: 0.56\n",
            "Average Accuracy: 0.56\n",
            "Average Precision: 0.56\n",
            "Average Recall: 0.56\n",
            "Average F1 Score: 0.56\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.72      0.69      0.71       122\n",
            "          A2       0.54      0.60      0.57       131\n",
            "          B1       0.48      0.58      0.53       118\n",
            "          B2       0.57      0.53      0.55       145\n",
            "          C1       0.43      0.36      0.39       121\n",
            "          C2       0.62      0.62      0.62       131\n",
            "\n",
            "    accuracy                           0.56       768\n",
            "   macro avg       0.56      0.56      0.56       768\n",
            "weighted avg       0.56      0.56      0.56       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TRY WITH AUGMENTED DATA GPT2"
      ],
      "metadata": {
        "id": "w9oGLAvvrokc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_df = pd.read_csv('https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/augmented_dataset.csv')"
      ],
      "metadata": {
        "id": "iabdtEw9rslH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_words(sentence):\n",
        "  '''\n",
        "  Function necessary to remove duplicated words in sentences generated by gpt-2 that made no sense.\n",
        "  '''\n",
        "  words = sentence.split()\n",
        "  seen = set()\n",
        "  unique_words = []\n",
        "  for word in words:\n",
        "      if word not in seen:\n",
        "          unique_words.append(word)\n",
        "          seen.add(word)\n",
        "  return ' '.join(unique_words)\n",
        "\n",
        "# Apply the function to the 'sentence' column\n",
        "augmented_df.loc[4800:, 'sentence'] = augmented_df.loc[4800:, 'sentence'].apply(remove_duplicate_words)"
      ],
      "metadata": {
        "id": "5u-adxgmrvA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fix the 'id' column\n",
        "def fix_id_column(df):\n",
        "    # Create a sequence of row numbers starting from 0\n",
        "    correct_ids = pd.Series(range(len(df)))\n",
        "    # Replace 'NaN' values and incorrect ids\n",
        "    df['id'] = correct_ids\n",
        "    return df\n",
        "\n",
        "# Fix the 'id' column\n",
        "augmented_df = fix_id_column(augmented_df)"
      ],
      "metadata": {
        "id": "nf0RHOM9ryL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from random import randint\n",
        "import random\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def synonym_replacement(sentence, n):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
        "    num_replaced = 0\n",
        "\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(random_word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "                synonyms.add(synonym)\n",
        "        if len(synonyms) > 1:\n",
        "            synonyms.discard(random_word)\n",
        "            synonym = list(synonyms)[randint(0, len(synonyms) - 1)]\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:  # only replace up to n words\n",
        "            break\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def shuffle_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    random.shuffle(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "augmented_data = []\n",
        "\n",
        "# Augment data\n",
        "for _, row in augmented_df.iterrows():\n",
        "    original_sentence = row['sentence']\n",
        "    difficulty = row['difficulty']\n",
        "\n",
        "    # Generate augmented sentences\n",
        "    augmented_sentence_synonym = synonym_replacement(original_sentence, 2)\n",
        "    augmented_sentence_shuffled = shuffle_sentence(original_sentence) #idk\n",
        "\n",
        "\n",
        "    # Append original and augmented sentences to the new list\n",
        "    augmented_data.append({'sentence': original_sentence, 'difficulty': difficulty})\n",
        "    augmented_data.append({'sentence': augmented_sentence_synonym, 'difficulty': difficulty})\n",
        "    augmented_data.append({'sentence': augmented_sentence_shuffled, 'difficulty': difficulty})\n",
        "\n",
        "\n",
        "# Create a new DataFrame from the augmented data\n",
        "data = pd.DataFrame(augmented_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrTpcB0Wr0by",
        "outputId": "805f58f7-b50d-4a96-d101-934531815ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map labels to integers\n",
        "label_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
        "data['difficulty'] = data['difficulty'].map(label_mapping)\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def get_sentence_length(sentence):\n",
        "    return len(sentence)\n",
        "\n",
        "def get_word_count(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return len(words)\n",
        "\n",
        "def get_avg_word_length(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return np.mean([len(word) for word in words])\n",
        "\n",
        "def count_punctuation(sentence):\n",
        "    return sum([1 for char in sentence if char in string.punctuation])\n",
        "\n",
        "# Extract Features\n",
        "data['sentence_length'] = data['sentence'].apply(get_sentence_length)\n",
        "data['word_count'] = data['sentence'].apply(get_word_count)\n",
        "data['avg_word_length'] = data['sentence'].apply(get_avg_word_length)\n",
        "data['punctuation_count'] = data['sentence'].apply(count_punctuation)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "feature_cols = ['sentence_length', 'word_count', 'avg_word_length', 'punctuation_count']\n",
        "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
        "\n",
        "train_encodings = tokenizer(X_train['sentence'].tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None, features=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        if self.features is not None:\n",
        "            item['features'] = torch.tensor(self.features[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Load pre-trained model\n",
        "\n",
        "\n",
        "# Cross-validation with KFold\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Training fold {fold + 1}\")\n",
        "\n",
        "    # Prepare training and validation datasets\n",
        "    train_data = data.iloc[train_idx]\n",
        "    val_data = data.iloc[val_idx]\n",
        "\n",
        "    model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=6).to(device)\n",
        "\n",
        "    train_encodings_fold = tokenizer(train_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "    val_encodings_fold = tokenizer(val_data['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    train_dataset = ClassificationDataset(train_encodings_fold, train_data['difficulty'].tolist(), train_data[feature_cols].values)\n",
        "    val_dataset = ClassificationDataset(val_encodings_fold, val_data['difficulty'].tolist(), val_data[feature_cols].values)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold + 1}',\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=1000,\n",
        "        weight_decay=0.005,\n",
        "        logging_dir=f'./logs_fold_{fold + 1}',\n",
        "        logging_steps=20,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=0.000015,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    # Define trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions, label_ids, metrics = trainer.predict(val_dataset)\n",
        "    preds = predictions.argmax(-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(val_data['difficulty'], preds)\n",
        "    precision = precision_score(val_data['difficulty'], preds, average='weighted')\n",
        "    recall = recall_score(val_data['difficulty'], preds, average='weighted')\n",
        "    f1 = f1_score(val_data['difficulty'], preds, average='weighted')\n",
        "\n",
        "    # Store metrics\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Fold {fold + 1} - Accuracy: {accuracy:.2f}')\n",
        "    print(f'Fold {fold + 1} - Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} - Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} - F1 Score: {f1:.2f}')\n",
        "\n",
        "# Print the average results across folds\n",
        "print(f'Average Accuracy: {np.mean(accuracy_list):.2f}')\n",
        "print(f'Average Precision: {np.mean(precision_list):.2f}')\n",
        "print(f'Average Recall: {np.mean(recall_list):.2f}')\n",
        "print(f'Average F1 Score: {np.mean(f1_list):.2f}')\n",
        "\n",
        "# Alternatively, print a detailed classification report for the final fold\n",
        "print(classification_report(val_data['difficulty'], preds, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))"
      ],
      "metadata": {
        "id": "VF4FPWnor7Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extend Df for A2 to C1\n"
      ],
      "metadata": {
        "id": "4He4eUpMRpDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = data[data['difficulty'].isin(['A2', 'B1', 'B2', 'C1'])]\n",
        "new_df = pd.concat([data, filtered_data])\n",
        "new_df = new_df.reset_index(drop=True)\n",
        "new_df['id'] = new_df.index"
      ],
      "metadata": {
        "id": "WIP_gRIqkXqs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "TOPfa5T4PdJ3",
        "outputId": "aa6bb388-836a-417b-b2aa-046644c81d16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                           sentence difficulty\n",
              "0        0  Les coûts kilométriques réels peuvent diverger...         C1\n",
              "1        1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
              "2        2  Le test de niveau en français est sur le site ...         A1\n",
              "3        3           Est-ce que ton mari est aussi de Boston?         A1\n",
              "4        4  Dans les écoles de commerce, dans les couloirs...         B1\n",
              "...    ...                                                ...        ...\n",
              "7975  7975  La réduction du dioxyde de carbone par l'eau n...         B2\n",
              "7976  7976  Elle connaissait à présent la petitesse des pa...         C1\n",
              "7977  7977  C'est pourquoi, il décida de remplacer les hab...         B2\n",
              "7978  7978  Il avait une de ces pâleurs splendides qui don...         C1\n",
              "7979  7979  Et le premier samedi de chaque mois, venez ren...         A2\n",
              "\n",
              "[7980 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c012b9d-a411-4ff5-b61c-06197621a4a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Le test de niveau en français est sur le site ...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
              "      <td>B1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7975</th>\n",
              "      <td>7975</td>\n",
              "      <td>La réduction du dioxyde de carbone par l'eau n...</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7976</th>\n",
              "      <td>7976</td>\n",
              "      <td>Elle connaissait à présent la petitesse des pa...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7977</th>\n",
              "      <td>7977</td>\n",
              "      <td>C'est pourquoi, il décida de remplacer les hab...</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7978</th>\n",
              "      <td>7978</td>\n",
              "      <td>Il avait une de ces pâleurs splendides qui don...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7979</th>\n",
              "      <td>7979</td>\n",
              "      <td>Et le premier samedi de chaque mois, venez ren...</td>\n",
              "      <td>A2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7980 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c012b9d-a411-4ff5-b61c-06197621a4a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2c012b9d-a411-4ff5-b61c-06197621a4a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2c012b9d-a411-4ff5-b61c-06197621a4a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d27699b2-c481-4893-9024-05855790abe9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d27699b2-c481-4893-9024-05855790abe9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d27699b2-c481-4893-9024-05855790abe9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "new_df",
              "summary": "{\n  \"name\": \"new_df\",\n  \"rows\": 7980,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2303,\n        \"min\": 0,\n        \"max\": 7979,\n        \"num_unique_values\": 7980,\n        \"samples\": [\n          2799,\n          2473,\n          7140\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4800,\n        \"samples\": [\n          \"Je peux m'asseoir ici ?\",\n          \"C'est la couleur de nombreux fruits et l\\u00e9gumes, comme les tomates, les fraises ou les cerises.\",\n          \"Pas au point qu'il faille en limiter la consommation, mais tout de m\\u00eame. D'autres \\u00e9diteurs exploitent le cr\\u00e9neau, et Internet regorge de sites int\\u00e9ressants qui permettent une approche moins scolaire, certes, mais tellement plus amusante, des math\\u00e9matiques.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difficulty\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"C1\",\n          \"A1\",\n          \"C2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT with augmented df"
      ],
      "metadata": {
        "id": "2uSKd1NV6vWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import accelerate\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "data = new_df  # Use the provided new_df\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
        "data['difficulty'] = data['difficulty'].map(label_mapping)\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def get_sentence_length(sentence):\n",
        "    return len(sentence)\n",
        "\n",
        "def get_word_count(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return len(words)\n",
        "\n",
        "def get_avg_word_length(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return np.mean([len(word) for word in words])\n",
        "\n",
        "def count_punctuation(sentence):\n",
        "    return sum([1 for char in sentence if char in string.punctuation])\n",
        "\n",
        "# Extract Features\n",
        "data['sentence_length'] = data['sentence'].apply(get_sentence_length)\n",
        "data['word_count'] = data['sentence'].apply(get_word_count)\n",
        "data['avg_word_length'] = data['sentence'].apply(get_avg_word_length)\n",
        "data['punctuation_count'] = data['sentence'].apply(count_punctuation)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "feature_cols = ['sentence_length', 'word_count', 'avg_word_length', 'punctuation_count']\n",
        "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "train_encodings = tokenizer(X_train['sentence'].tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None, features=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        if self.features is not None:\n",
        "            item['features'] = torch.tensor(self.features[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Load pre-trained model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=6).to(device)\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset = ClassificationDataset(train_encodings, y_train.tolist(), X_train[feature_cols].values)\n",
        "test_dataset = ClassificationDataset(test_encodings, y_test.tolist(), X_test[feature_cols].values)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.0005,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=0.000005,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
        "preds = predictions.argmax(-1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "precision = precision_score(y_test, preds, average='weighted')\n",
        "recall = recall_score(y_test, preds, average='weighted')\n",
        "f1 = f1_score(y_test, preds, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(classification_report(y_test, preds, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "cG3mN86prd9e",
        "outputId": "870ab84f-b7bf-4912-f6c9-5503743be622"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7980' max='7980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7980/7980 25:46, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.364300</td>\n",
              "      <td>1.308142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.139400</td>\n",
              "      <td>1.121433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.012600</td>\n",
              "      <td>0.996074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.781200</td>\n",
              "      <td>1.032601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.848600</td>\n",
              "      <td>0.914891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.677500</td>\n",
              "      <td>0.870037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.691400</td>\n",
              "      <td>0.843500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.555300</td>\n",
              "      <td>0.834816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.604500</td>\n",
              "      <td>0.824424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.396400</td>\n",
              "      <td>0.813546</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.72\n",
            "Precision: 0.72\n",
            "Recall: 0.72\n",
            "F1 Score: 0.72\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.71      0.57      0.63       169\n",
            "          A2       0.70      0.77      0.74       318\n",
            "          B1       0.74      0.78      0.76       329\n",
            "          B2       0.73      0.80      0.76       310\n",
            "          C1       0.75      0.76      0.76       312\n",
            "          C2       0.65      0.45      0.53       158\n",
            "\n",
            "    accuracy                           0.72      1596\n",
            "   macro avg       0.71      0.69      0.70      1596\n",
            "weighted avg       0.72      0.72      0.72      1596\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./distilbert')\n",
        "tokenizer.save_pretrained('./distilbert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dcjk705leDT",
        "outputId": "1fedf9c9-08fd-4892-9129-fa735f2539dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./distilbert/tokenizer_config.json',\n",
              " './distilbert/special_tokens_map.json',\n",
              " './distilbert/vocab.txt',\n",
              " './distilbert/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./distilbert.zip ./distilbert/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXm8LCKklouD",
        "outputId": "98f1e2e0-6a63-43c3-cb0e-33642cf02d0c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: distilbert/ (stored 0%)\n",
            "  adding: distilbert/tokenizer_config.json (deflated 75%)\n",
            "  adding: distilbert/vocab.txt (deflated 45%)\n",
            "  adding: distilbert/model.safetensors (deflated 7%)\n",
            "  adding: distilbert/special_tokens_map.json (deflated 42%)\n",
            "  adding: distilbert/config.json (deflated 52%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CamemBERT with augmented df"
      ],
      "metadata": {
        "id": "1Ww3mHWv613J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import CamembertModel, CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "data = new_df  # Use the provided new_df\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
        "data['difficulty'] = data['difficulty'].map(label_mapping)\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def get_sentence_length(sentence):\n",
        "    return len(sentence)\n",
        "\n",
        "def get_word_count(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return len(words)\n",
        "\n",
        "def get_avg_word_length(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return np.mean([len(word) for word in words])\n",
        "\n",
        "def count_punctuation(sentence):\n",
        "    return sum([1 for char in sentence if char in string.punctuation])\n",
        "\n",
        "# Extract Features\n",
        "data['sentence_length'] = data['sentence'].apply(get_sentence_length)\n",
        "data['word_count'] = data['sentence'].apply(get_word_count)\n",
        "data['avg_word_length'] = data['sentence'].apply(get_avg_word_length)\n",
        "data['punctuation_count'] = data['sentence'].apply(count_punctuation)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "feature_cols = ['sentence_length', 'word_count', 'avg_word_length', 'punctuation_count']\n",
        "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "train_encodings = tokenizer(X_train['sentence'].tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None, features=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        if self.features is not None:\n",
        "            item['features'] = torch.tensor(self.features[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Load pre-trained model\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6).to(device)\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset = ClassificationDataset(train_encodings, y_train.tolist(), X_train[feature_cols].values)\n",
        "test_dataset = ClassificationDataset(test_encodings, y_test.tolist(), X_test[feature_cols].values)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.0005,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=0.000005,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
        "preds = predictions.argmax(-1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "precision = precision_score(y_test, preds, average='weighted')\n",
        "recall = recall_score(y_test, preds, average='weighted')\n",
        "f1 = f1_score(y_test, preds, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(classification_report(y_test, preds, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "tDD4czHcc97B",
        "outputId": "10ab4301-199c-427c-bcfa-59610f600ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7980' max='7980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7980/7980 23:30, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.650800</td>\n",
              "      <td>1.601327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.301000</td>\n",
              "      <td>1.238415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.138400</td>\n",
              "      <td>1.101928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.908200</td>\n",
              "      <td>1.122260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.977600</td>\n",
              "      <td>0.982874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.788500</td>\n",
              "      <td>0.983709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.798500</td>\n",
              "      <td>0.998185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.796200</td>\n",
              "      <td>1.008876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.692500</td>\n",
              "      <td>0.955878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.630800</td>\n",
              "      <td>0.954595</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.63\n",
            "Precision: 0.64\n",
            "Recall: 0.63\n",
            "F1 Score: 0.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.73      0.64      0.68       169\n",
            "          A2       0.65      0.80      0.71       318\n",
            "          B1       0.67      0.69      0.68       329\n",
            "          B2       0.54      0.73      0.62       310\n",
            "          C1       0.61      0.50      0.55       312\n",
            "          C2       0.75      0.19      0.30       158\n",
            "\n",
            "    accuracy                           0.63      1596\n",
            "   macro avg       0.66      0.59      0.59      1596\n",
            "weighted avg       0.64      0.63      0.61      1596\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlauBERT with augmented df"
      ],
      "metadata": {
        "id": "oG6mT-Ls647w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import FlaubertModel, FlaubertTokenizer, FlaubertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "data = new_df  # Use the provided new_df\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
        "data['difficulty'] = data['difficulty'].map(label_mapping)\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def get_sentence_length(sentence):\n",
        "    return len(sentence)\n",
        "\n",
        "def get_word_count(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return len(words)\n",
        "\n",
        "def get_avg_word_length(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return np.mean([len(word) for word in words])\n",
        "\n",
        "def count_punctuation(sentence):\n",
        "    return sum([1 for char in sentence if char in string.punctuation])\n",
        "\n",
        "# Extract Features\n",
        "data['sentence_length'] = data['sentence'].apply(get_sentence_length)\n",
        "data['word_count'] = data['sentence'].apply(get_word_count)\n",
        "data['avg_word_length'] = data['sentence'].apply(get_avg_word_length)\n",
        "data['punctuation_count'] = data['sentence'].apply(count_punctuation)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "feature_cols = ['sentence_length', 'word_count', 'avg_word_length', 'punctuation_count']\n",
        "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, data['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
        "\n",
        "train_encodings = tokenizer(X_train['sentence'].tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test['sentence'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None, features=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        if self.features is not None:\n",
        "            item['features'] = torch.tensor(self.features[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Load pre-trained model\n",
        "model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=6).to(device)\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset = ClassificationDataset(train_encodings, y_train.tolist(), X_train[feature_cols].values)\n",
        "test_dataset = ClassificationDataset(test_encodings, y_test.tolist(), X_test[feature_cols].values)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=16,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.0005,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=0.000005,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
        "preds = predictions.argmax(-1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "precision = precision_score(y_test, preds, average='weighted')\n",
        "recall = recall_score(y_test, preds, average='weighted')\n",
        "f1 = f1_score(y_test, preds, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(classification_report(y_test, preds, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VYMUMRLtQ5G2",
        "outputId": "0de1b0f1-6c03-4887-dc99-ba70d6e0ffef"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12516' max='12768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12516/12768 34:29 < 00:41, 6.05 it/s, Epoch 15.68/16]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>1.525918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.273900</td>\n",
              "      <td>1.153823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.068600</td>\n",
              "      <td>1.035337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.776900</td>\n",
              "      <td>0.957850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.780900</td>\n",
              "      <td>0.938287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.585200</td>\n",
              "      <td>0.891768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.598600</td>\n",
              "      <td>0.855803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.436400</td>\n",
              "      <td>0.776548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.351300</td>\n",
              "      <td>0.871536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.263800</td>\n",
              "      <td>0.743021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.127700</td>\n",
              "      <td>0.700846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.147000</td>\n",
              "      <td>0.763696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.113400</td>\n",
              "      <td>0.826311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.129800</td>\n",
              "      <td>0.812104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.240700</td>\n",
              "      <td>0.844613</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12768' max='12768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12768/12768 35:11, Epoch 16/16]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>1.525918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.273900</td>\n",
              "      <td>1.153823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.068600</td>\n",
              "      <td>1.035337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.776900</td>\n",
              "      <td>0.957850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.780900</td>\n",
              "      <td>0.938287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.585200</td>\n",
              "      <td>0.891768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.598600</td>\n",
              "      <td>0.855803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.436400</td>\n",
              "      <td>0.776548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.351300</td>\n",
              "      <td>0.871536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.263800</td>\n",
              "      <td>0.743021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.127700</td>\n",
              "      <td>0.700846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.147000</td>\n",
              "      <td>0.763696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.113400</td>\n",
              "      <td>0.826311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.129800</td>\n",
              "      <td>0.812104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.240700</td>\n",
              "      <td>0.844613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.172600</td>\n",
              "      <td>0.844143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.82\n",
            "Precision: 0.83\n",
            "Recall: 0.82\n",
            "F1 Score: 0.82\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.75      0.67      0.71       169\n",
            "          A2       0.75      0.89      0.81       318\n",
            "          B1       0.86      0.83      0.85       329\n",
            "          B2       0.86      0.86      0.86       310\n",
            "          C1       0.87      0.89      0.88       312\n",
            "          C2       0.85      0.62      0.72       158\n",
            "\n",
            "    accuracy                           0.82      1596\n",
            "   macro avg       0.82      0.79      0.80      1596\n",
            "weighted avg       0.83      0.82      0.82      1596\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline for final prediction"
      ],
      "metadata": {
        "id": "mJfUJEoIJxpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, FlaubertForSequenceClassification, FlaubertTokenizer\n",
        "unlabelled_url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/unlabelled_test_data.csv\"\n",
        "unlabelled_data = pd.read_csv(unlabelled_url)\n",
        "\n",
        "# Load the saved CamemBERT model and tokenizer\n",
        "distilbert_path = './distilbert'\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained(distilbert_path)\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(distilbert_path)\n",
        "\n",
        "# Load the saved CamemBERT model and tokenizer\n",
        "camembert_path = './camembert'\n",
        "camembert_model = CamembertForSequenceClassification.from_pretrained(camembert_path)\n",
        "camembert_tokenizer = CamembertTokenizer.from_pretrained(camembert_path)\n",
        "\n",
        "# Load the saved Flaubert model and tokenizer\n",
        "flaubert_path = './flaubert'\n",
        "flaubert_model = FlaubertForSequenceClassification.from_pretrained(flaubert_path)\n",
        "flaubert_tokenizer = FlaubertTokenizer.from_pretrained(flaubert_path)\n",
        "\n",
        "# Create prediction pipelines to get probabilities\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "distilbert_classifier = pipeline('text-classification', model=distilbert_model, tokenizer=distilbert_tokenizer, framework='pt', device=device, return_all_scores=True)\n",
        "camembert_classifier = pipeline('text-classification', model=camembert_model, tokenizer=camembert_tokenizer, framework='pt', device=device, return_all_scores=True)\n",
        "flaubert_classifier = pipeline('text-classification', model=flaubert_model, tokenizer=flaubert_tokenizer, framework='pt', device=device, return_all_scores=True)\n",
        "\n",
        "# Predict probabilities for the unlabelled data using both models\n",
        "distilbert_probs = distilbert_classifier(unlabelled_data['sentence'].tolist())\n",
        "camembert_probs = camembert_classifier(unlabelled_data['sentence'].tolist())\n",
        "flaubert_probs = flaubert_classifier(unlabelled_data['sentence'].tolist())\n",
        "\n",
        "# Convert the predictions to numpy arrays\n",
        "distilbertprobs_array = np.array([[prob['score'] for prob in probs] for probs in distilbert_probs])\n",
        "camembert_probs_array = np.array([[prob['score'] for prob in probs] for probs in camembert_probs])\n",
        "flaubert_probs_array = np.array([[prob['score'] for prob in probs] for probs in flaubert_probs])\n",
        "\n",
        "# Combine predictions using soft voting (average probabilities)\n",
        "average_probs = (distilbertprobs_array + camembert_probs_array + flaubert_probs_array) / 3\n",
        "# average_probs = ( camembert_probs_array + flaubert_probs_array) / 2\n",
        "final_preds = np.argmax(average_probs, axis=1)\n",
        "S\n",
        "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "final_labels = [inverse_label_mapping[pred] for pred in final_preds]\n",
        "\n",
        "# Create a DataFrame to export\n",
        "results_df = pd.DataFrame({\n",
        "    'id': unlabelled_data['id'],\n",
        "    'difficulty': final_labels\n",
        "})\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "results_df.to_csv('s.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIt7ii40TKwE",
        "outputId": "3fc34fa6-9a10-4132-cf00-db81aa2372d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}