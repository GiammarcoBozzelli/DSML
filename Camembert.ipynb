{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiammarcoBozzelli/DSML/blob/main/Camembert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blnZsRGf6sIm",
        "outputId": "74b46b37-58eb-462d-fad1-589487a98488"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#imoprt packages\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IIijAIqq9Odd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import CamembertTokenizer, CamembertModel, pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1bFHb006wou",
        "outputId": "4d741cfb-1cc7-4768-9617-7adc393a58de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id                                           sentence difficulty\n",
            "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
            "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
            "2   2  Le test de niveau en français est sur le site ...         A1\n",
            "3   3           Est-ce que ton mari est aussi de Boston?         A1\n",
            "4   4  Dans les écoles de commerce, dans les couloirs...         B1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Preprocess the data: encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['Label'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['sentence'], data['Label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj_woHJK7oD5"
      },
      "outputs": [],
      "source": [
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertModel.from_pretrained(\"camembert-base\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1WyIGvoz9aKE"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(sentences):\n",
        "    # Ensure sentences are in a list and filter out any non-string entries\n",
        "    sentences = [s for s in sentences if isinstance(s, str)]\n",
        "\n",
        "    # Check if the batch is empty after filtering\n",
        "    if not sentences:\n",
        "        return np.array([])  # Return an empty array if no valid sentences are present\n",
        "\n",
        "    # Tokenize and encode the batch of sentences\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the embeddings from the last hidden state\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Batch processing of embeddings\n",
        "batch_size = 32  # Adjust batch size according to your GPU memory\n",
        "X_train_embeddings = np.vstack([get_embeddings(X_train[i:i + batch_size]) for i in range(0, len(X_train), batch_size) if len(X_train[i:i + batch_size]) > 0])\n",
        "X_test_embeddings = np.vstack([get_embeddings(X_test[i:i + batch_size]) for i in range(0, len(X_test), batch_size) if len(X_test[i:i + batch_size]) > 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nryavvuN7rCR",
        "outputId": "76f6ed46-e32b-4e33-8b2f-d78a349b38e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.525\n"
          ]
        }
      ],
      "source": [
        "# Create and train the logistic regression model\n",
        "from sklearn import svm\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test_embeddings)\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "npeG1R-w-7p3"
      },
      "outputs": [],
      "source": [
        "unlabelled_url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/unlabelled_test_data.csv\"\n",
        "unlabelled_data = pd.read_csv(unlabelled_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9FFD32aL-9Bj"
      },
      "outputs": [],
      "source": [
        "unlabelled_embeddings = np.vstack([get_embeddings(unlabelled_data['sentence'][i:i + batch_size]) for i in range(0, len(unlabelled_data), batch_size) if len(unlabelled_data['sentence'][i:i + batch_size]) > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZnAx3cA7_Dmr"
      },
      "outputs": [],
      "source": [
        "predicted_labels = clf.predict(unlabelled_embeddings)\n",
        "\n",
        "# Decode the predicted labels back to original labels\n",
        "decoded_labels = label_encoder.inverse_transform(predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JIqVGi2p_Upu"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({'id': unlabelled_data['id'], 'difficulty': decoded_labels})\n",
        "\n",
        "submission.to_csv('prediction1_DT.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN6PMppAB25cq/eTqrtpKqs",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
