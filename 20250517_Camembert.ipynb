{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiammarcoBozzelli/DSML/blob/main/20250517_Camembert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blnZsRGf6sIm",
        "outputId": "74b46b37-58eb-462d-fad1-589487a98488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#imoprt packages\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import CamembertTokenizer, CamembertModel, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "IIijAIqq9Odd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv\"\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1bFHb006wou",
        "outputId": "a9e2359f-8302-4fbd-f74e-9a6ea05c97a2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                           sentence difficulty\n",
            "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
            "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
            "2   2  Le test de niveau en français est sur le site ...         A1\n",
            "3   3           Est-ce que ton mari est aussi de Boston?         A1\n",
            "4   4  Dans les écoles de commerce, dans les couloirs...         B1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv')  # Update this with the correct path to your dataset\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"camembert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate sentences\n",
        "def generate_sentence(seed_text):\n",
        "    inputs = tokenizer(seed_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=200, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Generating new sentences for each difficulty level\n",
        "new_sentences = []\n",
        "for level in data['difficulty']:\n",
        "    # Randomly select a sentence from the same difficulty level\n",
        "    seed_text = data[data['difficulty'] == level].sample(1)['sentence'].iloc[0]\n",
        "    generated_sentence = generate_sentence(seed_text)\n",
        "    new_sentences.append({'sentence': generated_sentence, 'difficulty': level})\n",
        "\n",
        "# Append new sentences to the DataFrame\n",
        "new_df = pd.DataFrame(new_sentences)\n",
        "augmented_df = pd.concat([data, new_df], ignore_index=True)\n",
        "\n",
        "# Saving the augmented DataFrame\n",
        "# augmented_df.to_csv('augmented_dataset.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoQNDayCHC7h",
        "outputId": "a37db3c9-008a-4769-c43a-1c109420ce0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14400, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_df = pd.read_csv('https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/augmented_dataset.csv')"
      ],
      "metadata": {
        "id": "Xz8cLJILlMPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_words(sentence):\n",
        "  '''\n",
        "  Function necessary to remove duplicated words in sentences generated by gpt-2 that made no sense.\n",
        "  '''\n",
        "    words = sentence.split()\n",
        "    seen = set()\n",
        "    unique_words = []\n",
        "    for word in words:\n",
        "        if word not in seen:\n",
        "            unique_words.append(word)\n",
        "            seen.add(word)\n",
        "    return ' '.join(unique_words)\n",
        "\n",
        "# Apply the function to the 'sentence' column\n",
        "augmented_df.loc[4800:, 'sentence'] = augmented_df.loc[4800:, 'sentence'].apply(remove_duplicate_words)"
      ],
      "metadata": {
        "id": "HkCchG6alU8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fix the 'id' column\n",
        "def fix_id_column(df):\n",
        "    # Create a sequence of row numbers starting from 0\n",
        "    correct_ids = pd.Series(range(len(df)))\n",
        "    # Replace 'NaN' values and incorrect ids\n",
        "    df['id'] = correct_ids\n",
        "    return df\n",
        "\n",
        "# Fix the 'id' column\n",
        "augmented_df = fix_id_column(augmented_df)"
      ],
      "metadata": {
        "id": "p7DykcpwliJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from random import randint\n",
        "import random\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def synonym_replacement(sentence, n):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
        "    num_replaced = 0\n",
        "\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(random_word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "                synonyms.add(synonym)\n",
        "        if len(synonyms) > 1:\n",
        "            synonyms.discard(random_word)\n",
        "            synonym = list(synonyms)[randint(0, len(synonyms) - 1)]\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:  # only replace up to n words\n",
        "            break\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def shuffle_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    random.shuffle(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "augmented_data = []\n",
        "\n",
        "# Augment data\n",
        "for _, row in augmented_df.iterrows():\n",
        "    original_sentence = row['sentence']\n",
        "    difficulty = row['difficulty']\n",
        "\n",
        "    # Generate augmented sentences\n",
        "    augmented_sentence_synonym = synonym_replacement(original_sentence, 2)\n",
        "    augmented_sentence_shuffled = shuffle_sentence(original_sentence) #idk\n",
        "\n",
        "\n",
        "    # Append original and augmented sentences to the new list\n",
        "    augmented_data.append({'sentence': original_sentence, 'difficulty': difficulty})\n",
        "    augmented_data.append({'sentence': augmented_sentence_synonym, 'difficulty': difficulty})\n",
        "    augmented_data.append({'sentence': augmented_sentence_shuffled, 'difficulty': difficulty})\n",
        "\n",
        "\n",
        "# Create a new DataFrame from the augmented data\n",
        "data = pd.DataFrame(augmented_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBdGC6JGHBUl",
        "outputId": "a585d4b3-4777-4736-c6ed-a5438848fcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data: encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['Label'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['sentence'], data['Label'], test_size=0.05, random_state=42)"
      ],
      "metadata": {
        "id": "mDkqCzGZG5N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertModel.from_pretrained(\"camembert-base\").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj_woHJK7oD5",
        "outputId": "e0c39ea9-4f85-41f1-ee15-535facbfb370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(sentences):\n",
        "    # Ensure sentences are in a list and filter out any non-string entries\n",
        "    sentences = [s for s in sentences if isinstance(s, str)]\n",
        "\n",
        "    # Check if the batch is empty after filtering\n",
        "    if not sentences:\n",
        "        return np.array([])  # Return an empty array if no valid sentences are present\n",
        "\n",
        "    # Tokenize and encode the batch of sentences\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the embeddings from the last hidden state\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Batch processing of embeddings\n",
        "batch_size = 8  # Adjust batch size according to your GPU memory\n",
        "X_train_embeddings = np.vstack([get_embeddings(X_train[i:i + batch_size]) for i in range(0, len(X_train), batch_size) if len(X_train[i:i + batch_size]) > 0])\n",
        "X_test_embeddings = np.vstack([get_embeddings(X_test[i:i + batch_size]) for i in range(0, len(X_test), batch_size) if len(X_test[i:i + batch_size]) > 0])\n"
      ],
      "metadata": {
        "id": "1WyIGvoz9aKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test_embeddings)\n"
      ],
      "metadata": {
        "id": "nryavvuN7rCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred.round())\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAnaIBOfCcc9",
        "outputId": "e8f516a1-d454-4608-beb4-d0a8b80b53ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6208333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/unlabelled_test_data.csv\"\n",
        "unlabelled_data = pd.read_csv(unlabelled_url)"
      ],
      "metadata": {
        "id": "npeG1R-w-7p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_embeddings = np.vstack([get_embeddings(unlabelled_data['sentence'][i:i + batch_size]) for i in range(0, len(unlabelled_data), batch_size) if len(unlabelled_data['sentence'][i:i + batch_size]) > 0])"
      ],
      "metadata": {
        "id": "9FFD32aL-9Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = clf.predict(unlabelled_embeddings)\n",
        "\n",
        "# Decode the predicted labels back to original labels\n",
        "decoded_labels = label_encoder.inverse_transform(predicted_labels)"
      ],
      "metadata": {
        "id": "ZnAx3cA7_Dmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'id': unlabelled_data['id'], 'difficulty': decoded_labels})\n",
        "\n",
        "submission.to_csv('sub.csv', index=False)"
      ],
      "metadata": {
        "id": "JIqVGi2p_Upu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}