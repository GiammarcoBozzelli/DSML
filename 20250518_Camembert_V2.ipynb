{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiammarcoBozzelli/DSML/blob/wip/20250518_Camembert_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blnZsRGf6sIm",
        "outputId": "c9a9b84d-9d86-4fa0-ab80-686b9d72638c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#imoprt packages\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import CamembertTokenizer, CamembertModel, pipeline\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "IIijAIqq9Odd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/training_data.csv\"\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "p1bFHb006wou",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install necessary libraries\n",
        "# !pip install transformers torch\n",
        "\n",
        "# # Load model and tokenizer\n",
        "# model_name = \"camembert-base\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "import torch\n",
        "\n",
        "# camembert = torch.hub.load('/content/camembert-large/',model='sentencepiece.bpe.model')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load tokenizer and model\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/electra-base-french-europeana-cased-discriminator\")\n",
        "model = AutoModel.from_pretrained(\"dbmdz/electra-base-french-europeana-cased-discriminator\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgKPC4PCmST0",
        "outputId": "8fe7f646-c9f0-412a-de1f-c289d38aa0cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # Function to generate sentences\n",
        "# def generate_sentence(seed_text):\n",
        "#     inputs = tokenizer(seed_text, return_tensors=\"pt\")\n",
        "#     outputs = model.generate(inputs[\"input_ids\"], max_length=200, num_return_sequences=1)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# # Generating new sentences for each difficulty level\n",
        "# new_sentences = []\n",
        "# for level in data['difficulty']:\n",
        "#     # Randomly select a sentence from the same difficulty level\n",
        "#     seed_text = data[data['difficulty'] == level].sample(1)['sentence'].iloc[0]\n",
        "#     generated_sentence = generate_sentence(seed_text)\n",
        "#     new_sentences.append({'sentence': generated_sentence, 'difficulty': level})\n",
        "\n",
        "# # Append new sentences to the DataFrame\n",
        "# new_df = pd.DataFrame(new_sentences)\n",
        "# augmented_df = pd.concat([data, new_df], ignore_index=True)\n",
        "\n",
        "# # Saving the augmented DataFrame\n",
        "# # augmented_df.to_csv('augmented_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "MoQNDayCHC7h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_df = pd.read_csv('https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/augmented_dataset.csv')"
      ],
      "metadata": {
        "id": "Xz8cLJILlMPi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_words(sentence):\n",
        "  '''\n",
        "  Function necessary to remove duplicated words in sentences generated by gpt-2 that made no sense.\n",
        "  '''\n",
        "  words = sentence.split()\n",
        "  seen = set()\n",
        "  unique_words = []\n",
        "  for word in words:\n",
        "      if word not in seen:\n",
        "          unique_words.append(word)\n",
        "          seen.add(word)\n",
        "  return ' '.join(unique_words)\n",
        "\n",
        "# Apply the function to the 'sentence' column\n",
        "augmented_df.loc[4800:, 'sentence'] = augmented_df.loc[4800:, 'sentence'].apply(remove_duplicate_words)"
      ],
      "metadata": {
        "id": "HkCchG6alU8O"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fix the 'id' column\n",
        "def fix_id_column(df):\n",
        "    # Create a sequence of row numbers starting from 0\n",
        "    correct_ids = pd.Series(range(len(df)))\n",
        "    # Replace 'NaN' values and incorrect ids\n",
        "    df['id'] = correct_ids\n",
        "    return df\n",
        "\n",
        "# Fix the 'id' column\n",
        "augmented_df = fix_id_column(augmented_df)"
      ],
      "metadata": {
        "id": "p7DykcpwliJx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from random import randint\n",
        "import random\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def synonym_replacement(sentence, n):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
        "    num_replaced = 0\n",
        "\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(random_word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "                synonyms.add(synonym)\n",
        "        if len(synonyms) > 1:\n",
        "            synonyms.discard(random_word)\n",
        "            synonym = list(synonyms)[randint(0, len(synonyms) - 1)]\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:  # only replace up to n words\n",
        "            break\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def shuffle_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    random.shuffle(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "augmented_data = []\n",
        "\n",
        "# Augment data\n",
        "for _, row in augmented_df.iterrows():\n",
        "    original_sentence = row['sentence']\n",
        "    difficulty = row['difficulty']\n",
        "\n",
        "    # Generate augmented sentences\n",
        "    augmented_sentence_synonym = synonym_replacement(original_sentence, 2)\n",
        "    augmented_sentence_shuffled = shuffle_sentence(original_sentence) #idk\n",
        "\n",
        "\n",
        "    # Append original and augmented sentences to the new list\n",
        "    augmented_data.append({'sentence': original_sentence, 'difficulty': difficulty})\n",
        "    augmented_data.append({'sentence': augmented_sentence_synonym, 'difficulty': difficulty})\n",
        "    augmented_data.append({'sentence': augmented_sentence_shuffled, 'difficulty': difficulty})\n",
        "\n",
        "\n",
        "# Create a new DataFrame from the augmented data\n",
        "data = pd.DataFrame(augmented_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBdGC6JGHBUl",
        "outputId": "69c602b6-9889-4885-e9ce-ae99378df81c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data: encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['Label'] = label_encoder.fit_transform(data['difficulty'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['sentence'], data['Label'], test_size=0.05, random_state=42)"
      ],
      "metadata": {
        "id": "mDkqCzGZG5N6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertModel.from_pretrained(\"camembert-base\").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj_woHJK7oD5",
        "outputId": "a52159f3-758c-49ed-a798-6ae405367f77"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(sentences):\n",
        "    # Ensure sentences are in a list and filter out any non-string entries\n",
        "    sentences = [s for s in sentences if isinstance(s, str)]\n",
        "\n",
        "    # Check if the batch is empty after filtering\n",
        "    if not sentences:\n",
        "        return np.array([])  # Return an empty array if no valid sentences are present\n",
        "\n",
        "    # Tokenize and encode the batch of sentences\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the embeddings from the last hidden state\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Batch processing of embeddings\n",
        "batch_size = 32  # Adjust batch size according to your GPU memory\n",
        "X_train_embeddings = np.vstack([get_embeddings(X_train[i:i + batch_size]) for i in range(0, len(X_train), batch_size) if len(X_train[i:i + batch_size]) > 0])\n",
        "X_test_embeddings = np.vstack([get_embeddings(X_test[i:i + batch_size]) for i in range(0, len(X_test), batch_size) if len(X_test[i:i + batch_size]) > 0])\n"
      ],
      "metadata": {
        "id": "1WyIGvoz9aKE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test_embeddings)\n"
      ],
      "metadata": {
        "id": "nryavvuN7rCR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred.round())\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAnaIBOfCcc9",
        "outputId": "31029d43-48e5-4360-9cae-4297fd2a31f6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.66875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_url = \"https://raw.githubusercontent.com/GiammarcoBozzelli/DSML/main/DATA/unlabelled_test_data.csv\"\n",
        "unlabelled_data = pd.read_csv(unlabelled_url)"
      ],
      "metadata": {
        "id": "npeG1R-w-7p3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_embeddings = np.vstack([get_embeddings(unlabelled_data['sentence'][i:i + batch_size]) for i in range(0, len(unlabelled_data), batch_size) if len(unlabelled_data['sentence'][i:i + batch_size]) > 0])"
      ],
      "metadata": {
        "id": "9FFD32aL-9Bj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = clf.predict(unlabelled_embeddings)\n",
        "\n",
        "# Decode the predicted labels back to original labels\n",
        "decoded_labels = label_encoder.inverse_transform(predicted_labels)"
      ],
      "metadata": {
        "id": "ZnAx3cA7_Dmr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'id': unlabelled_data['id'], 'difficulty': decoded_labels})\n",
        "\n",
        "submission.to_csv('sub.csv', index=False)"
      ],
      "metadata": {
        "id": "JIqVGi2p_Upu"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}